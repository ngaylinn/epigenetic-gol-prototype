"""Run all experiments and rebuild result summaries to embed in README.md.

This project is a demo of a self-optimizing genetic algorithm. The README file
contains an overview of the experiment's purpose, design, and results. The
graphs, images, and videos referenced by the README are all produced by running
this script and stored in the output directory.

Here is an overview of the files generated by this script:
output
    # Results from experiments to evolve a GenomeConfig for every fitness goal.
    genome_experiments
        <fitness_goal>
            # Results using different parameters for GenomeConfig evolution.
            <variation_name>
                best_config.pickle
                best_config.txt
                best_simulation.gif
                best_trials.svg
                fitness.svg
                state.pickle
    # Examples of unevolved individuals for each GenomeConfig
    sample_initial_populations
        <genome_config>
            <simulation_id>.gif
    # Results from experiments run on a single GenomeConfig
    simulation_experiments
        <fitness_goal>
            # Comparisons between single-config experiments.
            compare_evolved_configs_cross_task.svg
            compare_evolved_variations.svg
            compare_predefined_to_evolved_configs.svg
            # The experiment results for each GenomeConfig.
            <genome_config>
                best_simulation.gif
                fitness.svg
                state.pickle
"""

import os
import pickle

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import experiments
import fitness
import genome
import gol_simulation


def make_title(name):
    """Reformat a snake case string into a title case string.

    This project uses snake case for filenames and dictionary keys. It uses the
    same name for chart titles and labels, but transformed with this function.
    """
    return name.replace('_', ' ').title().replace(' X ', ' x ')


def evolve_simulation(fitness_name, fitness_func, config_name, genome_config):
    """Evolve a simulation and record results of that experiment.

    This function runs a simulation experiment with the given fitness and
    GenomeConfig options, or reuses cached results if available. It saves a
    chart of fitness over generations for that experiment, as well as a video
    of the best simulation produced by that experiment. It returns the fitness
    data table for the experiment for comparison against similar experiments.
    """
    path = f'output/simulation_experiments/{fitness_name}/{config_name}'
    os.makedirs(path, exist_ok=True)
    data = experiments.simulation_experiment(
        f'{path}/state.pickle', f'{fitness_name}: {config_name}',
        fitness_func, genome_config)

    # Chart the results of this single-config experiment.
    fitness_data = data['fitness_data']
    fig = sns.relplot(data=fitness_data, kind='line',
                      x='Generation', y='Fitness', hue='Trial')
    fig.set(title=make_title(config_name))
    fig.savefig(f'{path}/fitness.svg')
    plt.close()

    # Save the best simulation from this experiment.
    best_simulation = data['best_simulation']
    best_simulation.save_video(f'{path}/best_simulation.gif')
    return fitness_data


def compare_configs(comparison_name, fitness_name, fitness_func, configs):
    """Compare the performance of simulations with different GenomeConfigs.

    This project explores how different GenomeConfigs make it easier or harder
    to evolve a good GameOfLifeSimulation for a variety of fitness goals. A key
    task is to compare the performance of several SimulationLineages, each
    configured with a different GenomeConfig, to see which one was better at
    learning or produced the best results.

    This function runs all the experiments for such a contest between
    GenomeConfigs (or restores their results from disk). It then generates a
    chart for each experiment summarizing the performance of a single
    GenomeConfig, records a video of that experiment's best simulation, and
    generates a chart comparing the results of all the experiments side by
    side.

    Parameters
    ----------
    comparison_name : str
        A brief description of used to name a chart that summarizes the
        comparison being made.
    per_fitness_configs : dict of str : dict of str : GenomeConfig
        A mapping from fitness goal to the set of GenomeConfigs to evaluate
        against that fitness goal. This is represented by a two-level tree of
        dictionaries. The outer dictionary is keyed by the name of the
        fitness_goal and the inner dictionary maps a name to a GenomeConfig.
    """
    path = f'output/simulation_experiments/{fitness_name}'
    all_fitness_data = pd.DataFrame()
    # Run / load data from all the simulation experiments to be compared and
    # aggregate their results into a single DataFrame.
    for config_name, genome_config in configs.items():
        fitness_data = evolve_simulation(
            fitness_name, fitness_func, config_name, genome_config)
        # Build up a DataFrame with the results from all experiments, keyed
        # by the GenomeConfig name.
        fitness_data['GenomeConfig'] = make_title(config_name)
        all_fitness_data = pd.concat((all_fitness_data, fitness_data))

    # Make sure the color coding is consistent between the two charts.
    hue_order = sorted(all_fitness_data['GenomeConfig'].unique())

    # Make a chart summarizing all the experiments side by side.
    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)
    fig.suptitle(make_title(comparison_name + f' for_{fitness_name}'))
    # On the left, we have median performance by generation for each config.
    sns.lineplot(
        data=all_fitness_data, x='Generation', y='Fitness',
        hue='GenomeConfig', ax=axes[0], hue_order=hue_order,
        # Draw a line for the median value, not the mean (which is default).
        # This is more representative, since it only includes real data points
        # found in our experiments, and not the extreme outliers.
        estimator=np.median,
        # Set the error bars to match the box part of the boxplot. This makes
        # it easier to correlate the two charts, but also gets rid of extreme
        # outlier data which made the first chart hard to read.
        errorbar=('pi', 50)
    ).set(title='Median By Generation')

    # In the middle we have a legend which serves as a reference for both
    # charts.
    sns.move_legend(
        axes[0], 'center left', bbox_to_anchor=(1, .5), frameon=False)

    # On the right, we have a boxplot comparing the max fitness score for each
    # GenomeConfig across its five trials.
    all_fitness_data = (
        all_fitness_data
        .groupby(['GenomeConfig', 'Trial'])
        .agg({'Fitness': 'max'})
        .reset_index())
    sns.boxplot(
        data=all_fitness_data, x='GenomeConfig', y='Fitness', ax=axes[1],
        hue_order=hue_order
    ).set(title='Max Across Trials',
          # Squeezing the GenomeConfig labels into the x axis doesn't work, so
          # just use the order / color code to indicate which is which.
          xticklabels=[])

    # Matplotlib hides the tickmarks for the y axis of the second chart by
    # default, since it's shared with the first chart. Force those back on for
    # improved readability.
    axes[1].yaxis.set_tick_params(labelleft=True)

    plt.tight_layout()
    fig.savefig(f'{path}/{comparison_name}.svg')
    plt.close()


def evolve_config(fitness_name, fitness_func, variation_name, variation):
    """Evolves GenomeConfigs for a fitness function, with particular settings.

    This project evolves GenomeConfigs suited to particular fitness goals. It
    does this using three distinct techniques: tuning global mutation and
    crossover rates, using a FitnessVector to condition those rates on recent
    performance, and tuning mutation rates on a fine-grain, gene-by-gene basis.

    Evovled global mutation and crossover rates are not unheard-of in the world
    of genetic algorithms. For this project, they are considered baseline
    behavior for an evolved GenomeConfig against which the other techniques are
    compared. For that reason, we test four variations to measure the impact of
    FitnessVectors and fine-grain mutations, independently and together.

    This function is used to evolve a GenomeConfig for one of the four
    variations under consideration, to be compared against the others.
    """
    path = f'output/genome_experiments/{fitness_name}/{variation_name}'
    os.makedirs(path, exist_ok=True)
    print('Preparing to run genome experiment '
          f'{fitness_name}, {variation_name}')
    data = experiments.genome_experiment(
        f'{path}/state.pickle', f'{fitness_name}: {variation_name}',
        fitness_func, *variation)

    # Record the best GenomeConfig found.
    best_config = data['best_config']
    with open(f'{path}/best_config.pickle', 'wb') as file:
        pickle.dump(best_config, file)
    with open(f'{path}/best_config.txt', 'w', encoding='utf-8') as file:
        file.write(str(best_config))

    # Chart fitness for this GenomeLineage experiment.
    fig = sns.relplot(data=data['fitness_data'], kind='line',
                      x='Generation', y='Fitness')
    fig.set(title=make_title(variation_name))
    fig.savefig(f'{path}/fitness.svg')
    plt.close()

    # Chart fitness for all the SimulationLineage trials run for the best
    # evolved GenomeConfig
    fig = sns.relplot(data=data['best_fitness_data'], kind='line',
                      x='Generation', y='Fitness', hue='Trial')
    fig.set(
        title=make_title(f'Best Trials ({make_title(variation_name)})'))
    fig.savefig(f'{path}/best_trials.svg')
    plt.close()

    # Record a video of the best simulation found for this GenomeConfig.
    best_simulation = data['best_simulation']
    best_simulation.save_video(f'{path}/best_simulation.gif')
    return best_config


def make_sample_populations(genome_configs):
    """Save samples of random simulations made with different GenomeConfigs.

    This function is used to show what the initial population of simulations
    from each GenomeConfig looks before it gets evolved by a SimulationLineage.
    """
    for config_name, genome_config in genome_configs.items():
        path = f'output/sample_initial_populations/{config_name}'
        os.makedirs(path, exist_ok=True)
        # This is mostly to remove any discrepencies that might come from
        # running this function multiple times with different arguments.
        experiments.reset_global_state()
        population = [
            gol_simulation.GameOfLifeSimulation(genome.Genotype(genome_config))
            for _ in range(experiments.POPULATION_SIZE)]
        gol_simulation.simulate(population)
        for simulation in population:
            simulation.save_video(f'{path}/{simulation.identifier}.gif')


def main():
    """Run experiments and summarize their results in the output directory."""
    sns.set_theme()

    # Select which fitness goals to analyze.
    fitness_goals = {
        name: func for name, func in fitness.ALL_GOALS.items()
        if name in ['explode', 'left_to_right']} #, 'symmetry', 'two_cycle', 'three_cycle']}

    # Evolve GenomeConfigs for every fitness goal, with and without fitness
    # vectors and fine-grain mutations enabled.
    evolved_configs = {}
    for fitness_name, fitness_func in fitness_goals.items():
        variant_configs = {}
        for variation_name, variation in experiments.CONFIG_VARIATIONS.items():
            variant_configs[variation_name] = evolve_config(
                fitness_name, fitness_func, variation_name, variation)

        # Compare the performance of the GenomeConfigs evolved with and without
        # fitness vectors and fine-grain mutations. This shows the effect those
        # variations had on the genetic algorithm, separately and together.
        compare_configs('compare_evolved_variations', fitness_name,
                        fitness_func, variant_configs)

        # Save the evolved GenomeConfig with everything enabled to use as the
        # standard to compare against the predefined GenomeConfigs.
        evolved_config = variant_configs['vector_x_fine-grain']
        evolved_configs[f'evolved_for_{fitness_name}'] = evolved_config

    # Compare the performance of the best evolved GenomeConfig for each fitness
    # goal on all of the fitness goals, including the ones it wasn't evolved
    # for. This shows how task-specific each GenomeConfig has evolved to be.
    for fitness_name, fitness_func in fitness_goals.items():
        compare_configs('compare_evolved_configs_cross_task',
                        fitness_name, fitness_func, evolved_configs)

    # For each fitness function, compare the performance of the predefined
    # GenomeConfigs against the best one evolved for that fitness goal.
    for fitness_name, fitness_func in fitness_goals.items():
        configs = genome.PREDEFINED_CONFIGS | {
            config_name: evolved_config
            for config_name, evolved_config in evolved_configs.items()
            if fitness_name in config_name
        }
        compare_configs('compare_predefined_to_evolved_configs',
                        fitness_name, fitness_func, configs)

    # Save an example initial population to document the behavior of the
    # GenomeConfigs evaluated above.
    print('Saving sample populations')
    make_sample_populations(genome.PREDEFINED_CONFIGS | evolved_configs)


if __name__ == '__main__':
    main()
