"""Run all experiments and rebuild result summaries to embed in README.md.

This project is a demo of a self-optimizing genetic algorithm. The README file
contains an overview of the experiment's purpose, design, and results. The
graphs, images, and videos referenced by the README are all produced by running
this script and stored in the output directory.

Here is an overview of the files generated by this script:
output
    # Results from experiments to evolve a GenomeConfig for every fitness goal.
    genome_experiments
        <fitness_goal>
            # Results using different parameters for GenomeConfig evolution.
            <variation_name>
                best_config.pickle
                best_config.txt
                best_simulation.gif
                best_trials.svg
                fitness.svg
                state.pickle
    # Examples of unevolved individuals for each GenomeConfig
    sample_initial_populations
        <genome_config>
            <simulation_id>.gif
    # Results from experiments run on a single GenomeConfig
    simulation_experiments
        <fitness_goal>
            # Comparisons between single-config experiments.
            compare_evolved_configs_cross_task.svg
            compare_evolved_variations.svg
            compare_predefined_to_evolved_configs.svg
            # The experiment results for each GenomeConfig.
            <genome_config>
                best_simulation.gif
                fitness.svg
                state.pickle
            # The best simulation found.
            best_by_<genome_config>.gif
"""

import functools
import glob
import os
import pickle
import shutil

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import experiments
import fitness
import genome
import gol_simulation


@functools.cache
def make_title(name):
    """Reformat a snake case string into a title case string.

    This project uses snake case for filenames and dictionary keys. It uses the
    same name for chart titles and labels, but transformed with this function.
    """
    return name.replace('_', ' ').title().replace(' X ', ' x ')


def evolve_simulation(fitness_name, fitness_func, config_name, genome_config):
    """Evolve a simulation and record results of that experiment.

    This function runs a simulation experiment with the given fitness and
    GenomeConfig options, or reuses cached results if available. It saves a
    chart of fitness over generations for that experiment, as well as a video
    of the best simulation produced by that experiment. It returns the fitness
    data table for the experiment for comparison against similar experiments.
    """
    path = f'output/simulation_experiments/{fitness_name}/{config_name}'
    os.makedirs(path, exist_ok=True)
    data = experiments.simulation_experiment(
        f'{path}/state.pickle', f'{fitness_name}: {config_name}',
        fitness_func, genome_config)

    filename = f'{path}/fitness.svg'
    fitness_data = data['fitness_data']
    if not os.path.exists(filename):
        # Chart the results of this single-config experiment.
        fig = sns.relplot(data=fitness_data, kind='line',
                          x='Generation', y='Fitness', hue='Trial')
        fig.set(title=make_title(config_name + f'_for_{fitness_name}'))
        fig.savefig(filename)
        plt.close()

    # Save the best simulation from this experiment.
    best_simulation = data['best_simulation']
    filename = f'{path}/best_simulation_{best_simulation.fitness}.gif'
    if not os.path.exists(filename):
        best_simulation.save_video(filename)

    return fitness_data


def compare_configs(comparison_name, fitness_name, fitness_func, configs):
    """Compare the performance of simulations with different GenomeConfigs.

    This project explores how different GenomeConfigs make it easier or harder
    to evolve a good GameOfLifeSimulation for a variety of fitness goals. A key
    task is to compare the performance of several SimulationLineages, each
    configured with a different GenomeConfig, to see which one was better at
    learning or produced the best results.

    This function runs all the experiments for such a contest between
    GenomeConfigs (or restores their results from disk). It then generates a
    chart for each experiment summarizing the performance of a single
    GenomeConfig, records a video of that experiment's best simulation, and
    generates a chart comparing the results of all the experiments side by
    side.

    Parameters
    ----------
    comparison_name : str
        A brief description of used to name a chart that summarizes the
        comparison being made.
    per_fitness_configs : dict of str : dict of str : GenomeConfig
        A mapping from fitness goal to the set of GenomeConfigs to evaluate
        against that fitness goal. This is represented by a two-level tree of
        dictionaries. The outer dictionary is keyed by the name of the
        fitness_goal and the inner dictionary maps a name to a GenomeConfig.

    Returns
    -------
    dict of str : float
        A map from config name to a single float representing the performance
        of that configuration. The score is computed by finding the max fitness
        score from each trial and taking the median of those.
    """
    all_fitness_data = pd.DataFrame()
    # Run / load data from all the simulation experiments to be compared and
    # aggregate their results into a single DataFrame.
    for config_name, genome_config in configs.items():
        fitness_data = evolve_simulation(
            fitness_name, fitness_func, config_name, genome_config)
        # Build up a DataFrame with the results from all experiments, keyed
        # by the GenomeConfig name.
        fitness_data['GenomeConfig'] = make_title(config_name)
        all_fitness_data = pd.concat((all_fitness_data, fitness_data))

    # For each generation of each trial, find the median fitness across the
    # population. Then, take the maximum across generations. This represents
    # the "best performance" of that trial, once outliers are discarded. Keep
    # in mind that after evolving a population for many generations, the range
    # of fitness scores should be quite narrow, so the median and the max
    # generally aren't far apart.
    per_trial_performance = (
        all_fitness_data
        .groupby(['GenomeConfig', 'Trial', 'Generation'])
        .agg({'Fitness': np.median})
        .groupby(['GenomeConfig', 'Trial'])
        .agg({'Fitness': np.max})
        .reset_index())

    # Compute an overall score for how this config did across trials. For this,
    # we look at the best performance per trial calculated above, and take the
    # median across all trials.
    config_scores = {
        config_name: (
            per_trial_performance
            .where(per_trial_performance['GenomeConfig'] ==
                   make_title(config_name))
            .get('Fitness')
            .median())
        for config_name in configs
    }

    filename = (f'output/simulation_experiments/{fitness_name}/'
                f'{comparison_name}.svg')
    if os.path.exists(filename):
        return config_scores

    # Make sure the coloring / legend are consistent between the two charts.
    hue_order = sorted([make_title(config_name) for config_name in configs])

    # Make a side-by-side chart that compares the experiment data from the
    # configurations in two ways.
    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)
    fig.suptitle(make_title(comparison_name + f' for_{fitness_name}'))
    # On the left, we have median performance by generation for each config.
    sns.lineplot(
        data=all_fitness_data, x='Generation', y='Fitness',
        hue='GenomeConfig', ax=axes[0], hue_order=hue_order,
        # Draw a line for the median value, not the mean (which is default).
        # This is more representative, since it only includes real data points
        # found in our experiments, and not the extreme outliers.
        estimator=np.median,
        # Set the error bars to match the box part of the boxplot. This makes
        # it easier to correlate the two charts, but also gets rid of extreme
        # outlier data which made the first chart hard to read.
        errorbar=('pi', 50)
    ).set(title='Median By Generation')

    # In the middle we have a legend which serves as a reference for both
    # charts.
    sns.move_legend(
        axes[0], 'center left', bbox_to_anchor=(1, .5), frameon=False)

    # On the right, we have a boxplot showing the best performance of each
    # config (as computed above) and how much that varied across trials.
    sns.boxplot(
        data=per_trial_performance, x='GenomeConfig', y='Fitness', ax=axes[1],
        hue_order=hue_order
    ).set(title='Best Performance Across Trials',
          # Squeezing the GenomeConfig labels into the x axis doesn't work, so
          # just use the order / color code to indicate which is which.
          xticklabels=[])

    # Matplotlib hides the tickmarks for the y axis of the second chart by
    # default, since it's shared with the first chart. Force those back on for
    # improved readability.
    axes[1].yaxis.set_tick_params(labelleft=True)

    # Resolve the comparison chart and save it.
    plt.tight_layout()
    fig.savefig(filename)
    plt.close()

    # Return a dict with the aggregate best performance score for each config.
    return config_scores


def evolve_config(fitness_name, fitness_func, variation_name, variation):
    """Evolves GenomeConfigs for a fitness function, with particular settings.

    This project evolves GenomeConfigs suited to particular fitness goals. It
    does this using three distinct techniques: tuning global mutation and
    crossover rates, using a FitnessVector to condition those rates on recent
    performance, and tuning mutation rates on a fine-grain, gene-by-gene basis.

    Evovled global mutation and crossover rates are not unheard-of in the world
    of genetic algorithms. For this project, they are considered baseline
    behavior for an evolved GenomeConfig against which the other techniques are
    compared. For that reason, we test four variations to measure the impact of
    FitnessVectors and fine-grain mutations, independently and together.

    This function is used to evolve a GenomeConfig for one of the four
    variations under consideration, to be compared against the others.
    """
    path = f'output/genome_experiments/{fitness_name}/{variation_name}'
    os.makedirs(path, exist_ok=True)
    data = experiments.genome_experiment(
        f'{path}/state.pickle', f'{fitness_name}: {variation_name}',
        fitness_func, *variation)

    # Record the best GenomeConfig found.
    best_config = data['best_config']
    filename = f'{path}/best_config.pickle'
    if not os.path.exists(filename):
        with open(filename, 'wb') as file:
            pickle.dump(best_config, file)
    filename = f'{path}/best_config.txt'
    if not os.path.exists(filename):
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(str(best_config))

    # Chart fitness for this GenomeLineage experiment.
    filename = f'{path}/fitness.svg'
    if not os.path.exists(filename):
        fig = sns.relplot(data=data['fitness_data'], kind='line',
                          x='Generation', y='Fitness')
        fig.set(title=make_title(variation_name))
        fig.savefig(filename)
        plt.close()

    # Chart fitness for all the SimulationLineage trials run for the best
    # evolved GenomeConfig
    filename = f'{path}/best_trials.svg'
    if not os.path.exists(filename):
        fig = sns.relplot(data=data['best_fitness_data'], kind='line',
                          x='Generation', y='Fitness', hue='Trial')
        fig.set(
            title=make_title(f'Best Trials ({make_title(variation_name)})'))
        fig.savefig(filename)
        plt.close()

    # Record a video of the best simulation found for this GenomeConfig.
    filename = f'{path}/best_simulation.gif'
    if not os.path.exists(filename):
        best_simulation = data['best_simulation']
        best_simulation.save_video(filename)
    return best_config


def make_sample_populations(genome_configs):
    """Save samples of random simulations made with different GenomeConfigs.

    This function is used to show what the initial population of simulations
    from each GenomeConfig looks before it gets evolved by a SimulationLineage.
    """
    for config_name, genome_config in genome_configs.items():
        path = f'output/sample_initial_populations/{config_name}'
        os.makedirs(path, exist_ok=True)
        # This is mostly to remove any discrepencies that might come from
        # running this function multiple times with different arguments.
        experiments.reset_global_state()
        population = [
            gol_simulation.GameOfLifeSimulation(genome.Genotype(genome_config))
            for _ in range(experiments.POPULATION_SIZE)]
        gol_simulation.simulate(population)
        for simulation in population:
            simulation.save_video(f'{path}/{simulation.identifier}.gif')


def main():
    """Run experiments and summarize their results in the output directory."""
    sns.set_theme()

    # Select which fitness goals to analyze.
    fitness_goals = fitness.ALL_GOALS

    # Evolve GenomeConfigs for every fitness goal, with and without fitness
    # vectors and fine-grain mutations enabled.
    print('Evolving GenomeConfigs for every fitness goal')
    evolved_configs = {}
    evolved_config_data = pd.DataFrame()
    for fitness_name, fitness_func in fitness_goals.items():
        variant_configs = {}
        for variation_name, variation in experiments.CONFIG_VARIATIONS.items():
            variant_config = evolve_config(
                fitness_name, fitness_func, variation_name, variation)
            config_data = variant_config.to_data_frame()
            config_data['use_fitness_vector'] = variation[0]
            config_data['use_per_gene_config'] = variation[1]
            config_data['fitness_goal'] = fitness_name
            evolved_config_data = pd.concat((
                evolved_config_data, config_data))
            variant_configs[variation_name] = variant_config

        # Compare the performance of the GenomeConfigs evolved with and without
        # fitness vectors and fine-grain mutations. This shows the effect those
        # variations had on the genetic algorithm, separately and together.
        config_scores = compare_configs('compare_evolved_variations',
                                        fitness_name, fitness_func,
                                        variant_configs)
        best_variant = max(config_scores, key=config_scores.get)

        # Compare the GenomeConfig with simple evolved mutation and crossover
        # rates against the Control configuration, which is its closest
        # predefined equivalent.
        compare_configs(
            'compare_default_to_evolved_rates', fitness_name, fitness_func, {
                'no_vector_x_global_only': variant_configs[
                    'no_vector_x_global_only'],
                'control': genome.PREDEFINED_CONFIGS['control']})

        # Save the evolved GenomeConfig with everything enabled to use as the
        # standard to compare against the predefined GenomeConfigs.
        evolved_config = variant_configs[best_variant]
        evolved_configs[f'evolved_for_{fitness_name}'] = evolved_config

    # Save a summary of all the evolved configs in a CSV file for easy analysis
    # in a spreadsheet tool.
    filename = 'output/genome_experiments/config_data.csv'
    if not os.path.exists(filename):
        evolved_config_data.to_csv(filename)

    print('Comparing all GenomeConfigs head-to-head')
    # Compare the performance of the best evolved GenomeConfig for each fitness
    # goal on all of the fitness goals, including the ones it wasn't evolved
    # for. This shows how task-specific each GenomeConfig has evolved to be.
    for fitness_name, fitness_func in fitness_goals.items():
        # Note, configs evovled for a different fitness goal are excluded from
        # the final head-to-head comparison below. If they were better, that's
        # coincidental, and doesn't count as the algorithm being successful.
        compare_configs('compare_evolved_configs_cross_task', fitness_name,
                        fitness_func, evolved_configs)

    # For each fitness function, compare the performance of the predefined
    # GenomeConfigs against the best one evolved for that fitness goal.
    for fitness_name, fitness_func in fitness_goals.items():
        configs = genome.PREDEFINED_CONFIGS | {
            config_name: evolved_config
            for config_name, evolved_config in evolved_configs.items()
            if fitness_name in config_name
        }
        compare_configs(
            'compare_predefined_to_evolved_configs', fitness_name,
            fitness_func, configs)

    # Save an example initial population to document the behavior of the
    # GenomeConfigs evaluated above.
    print('Saving sample populations')
    make_sample_populations(genome.PREDEFINED_CONFIGS | evolved_configs)


if __name__ == '__main__':
    main()
